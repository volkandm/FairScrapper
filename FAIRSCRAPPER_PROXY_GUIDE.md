# ğŸ”’ FairScrapper Proxy KullanÄ±m KÄ±lavuzu

Bu kÄ±lavuz, FairScrapper projenizde proxy kullanÄ±mÄ±nÄ± aÃ§Ä±klar.

## ğŸ“‹ Ä°Ã§indekiler

1. [Ãœcretsiz Proxy SeÃ§enekleri](#Ã¼cretsiz-proxy-seÃ§enekleri)
2. [Proxy Bulma](#proxy-bulma)
3. [Proxy Test Etme](#proxy-test-etme)
4. [Proxy ile Scraping](#proxy-ile-scraping)
5. [GÃ¼venlik Ã–nerileri](#gÃ¼venlik-Ã¶nerileri)

## ğŸ†“ Ãœcretsiz Proxy SeÃ§enekleri

### 1. **Free Proxy List'ler**
- **free-proxy-list.net** - En popÃ¼ler
- **proxynova.com** - GÃ¼ncel listeler
- **proxy-list.download** - HÄ±zlÄ± proxy'ler

### 2. **Tor Network (En GÃ¼venli)**
```bash
# Tor browser yÃ¼kle
brew install tor-browser  # macOS
# veya https://www.torproject.org/ adresinden indir
```

### 3. **Test Proxy'leri**
```env
# Test iÃ§in
PROXY_LIST=http://httpbin.org:80
```

## ğŸ” Proxy Bulma

### Otomatik Proxy Bulucu

```bash
# Ãœcretsiz proxy'leri bul ve test et
python free_proxy_finder.py
```

**Ã‡Ä±ktÄ±:**
```
âœ… Found 3 working proxies:
1. http://57.129.81.201:8080 (DE)
2. http://176.126.103.194:44214 (RU)
3. http://158.69.185.37:3129 (CA)
```

### Manuel Proxy Bulma

1. **free-proxy-list.net** adresine git
2. HTTPS proxy'leri filtrele
3. IP ve port bilgilerini kopyala
4. Test et

## ğŸ§ª Proxy Test Etme

### HÄ±zlÄ± Test

```bash
# Bulunan proxy'leri test et
python proxy_test.py
```

### Manuel Test

```python
from scraper import WebScraper
import asyncio

async def test_proxy():
    scraper = WebScraper()
    # Proxy will be loaded from environment variables
    
    await scraper.setup_browser()
    await scraper.navigate_to_url("https://httpbin.org/ip")
    ip = await scraper.get_element_text("pre")
    print(f"IP: {ip}")
    await scraper.close()

asyncio.run(test_proxy())
```

## ğŸš€ Proxy ile Scraping

### 1. .env DosyasÄ± ile

```env
# .env dosyasÄ±
PROXY_ENABLED=true
PROXY_LIST=http://57.129.81.201:8080
```

### 2. Kod ile

```python
from scraper import WebScraper

scraper = WebScraper()
# Proxy automatically loaded from environment variables
# Proxy will be loaded from environment variables

await scraper.setup_browser()
# Normal scraping iÅŸlemleri...
```

### 3. Ã‡oklu Proxy

```python
# .env dosyasÄ±nda tanÄ±mla
PROXY_LIST=http://57.129.81.201:8080,http://158.69.185.37:3129

# Kod otomatik olarak proxy rotasyonu yapar
scraper = WebScraper()
await scraper.setup_browser()
# Scraping iÅŸlemi...
```

## ğŸ”’ GÃ¼venlik Ã–nerileri

### âœ… YapÄ±lacaklar

1. **Proxy Rotasyonu**
   ```python
   import random
   
   proxy_list = ["proxy1", "proxy2", "proxy3"]
   selected_proxy = random.choice(proxy_list)
   ```

2. **Rate Limiting**
   ```python
   import asyncio
   
   await asyncio.sleep(2)  # 2 saniye bekle
   ```

3. **User-Agent DeÄŸiÅŸtirme**
   ```python
   user_agents = [
       "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0",
       "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Safari/605.1.15"
   ]
   ```

4. **Timeout AyarlarÄ±**
   ```python
   scraper.config.TIMEOUT = 30000  # 30 saniye
   ```

### âŒ YapÄ±lmayacaklar

1. **AynÄ± proxy'yi sÃ¼rekli kullanma**
2. **Ã‡ok hÄ±zlÄ± istek gÃ¶nderme**
3. **GÃ¼venilir olmayan proxy'ler kullanma**
4. **KiÅŸisel bilgileri proxy ile gÃ¶nderme**

## ğŸ“Š Proxy PerformansÄ±

### Test SonuÃ§larÄ±

| Proxy | HÄ±z | GÃ¼venilirlik | Ãœlke |
|-------|-----|--------------|------|
| 57.129.81.201:8080 | â­â­â­ | â­â­â­â­ | DE |
| 158.69.185.37:3129 | â­â­â­â­ | â­â­â­ | CA |

### Ã–nerilen KullanÄ±m

```env
# En iyi proxy'leri Ã¶nce kullan
PROXY_LIST=http://57.129.81.201:8080,http://158.69.185.37:3129
```

## ğŸ› ï¸ Sorun Giderme

### YaygÄ±n Hatalar

1. **Connection Refused**
   ```
   âŒ Proxy artÄ±k Ã§alÄ±ÅŸmÄ±yor
   âœ… Yeni proxy bul
   ```

2. **Timeout**
   ```
   âŒ Proxy yavaÅŸ
   âœ… Timeout sÃ¼resini artÄ±r
   ```

3. **SSL Errors**
   ```
   âŒ HTTPS sorunu
   âœ… HTTP proxy kullan
   ```

### Debug KomutlarÄ±

```bash
# Proxy test
python proxy_test.py

# IP kontrolÃ¼
python -c "
import requests
proxies = {'http': 'http://proxy:port'}
r = requests.get('https://httpbin.org/ip', proxies=proxies)
print(r.json())
"
```

## ğŸ“ Ã–rnek KullanÄ±m

### Basit Ã–rnek

```python
from scraper import WebScraper
import asyncio

async def scrape_with_proxy():
    scraper = WebScraper()
    # Proxy will be loaded from environment variables
    
    await scraper.setup_browser()
    await scraper.navigate_to_url("https://example.com")
    
    title = await scraper.get_element_text("h1")
    print(f"Title: {title}")
    
    await scraper.take_screenshot("screenshot.png")
    await scraper.close()

asyncio.run(scrape_with_proxy())
```

### GeliÅŸmiÅŸ Ã–rnek

```python
import asyncio
from scraper import WebScraper

async def smart_scraping():
    scraper = WebScraper()
    # Proxy list will be loaded from environment variables
    # Automatic rotation and fallback is handled by the scraper
    
    try:
        await scraper.setup_browser()
        
        # Scraping iÅŸlemleri...
        await scraper.navigate_to_url("https://target-site.com")
        
        # Rate limiting
        await asyncio.sleep(2)
        
    except Exception as e:
        print(f"Scraping failed: {e}")
    finally:
        await scraper.close()

asyncio.run(smart_scraping())
```

## ğŸ¯ SonuÃ§

Proxy kullanÄ±mÄ± web scraping'de Ã¶nemlidir:

- âœ… **IP gizleme**
- âœ… **Rate limiting bypass**
- âœ… **CoÄŸrafi kÄ±sÄ±tlamalarÄ± aÅŸma**
- âœ… **Bot tespitini zorlaÅŸtÄ±rma**

**Ã–nerilen yaklaÅŸÄ±m:** Ãœcretsiz proxy'leri test edin, Ã§alÄ±ÅŸanlarÄ± kaydedin ve rotasyon yapÄ±n. 